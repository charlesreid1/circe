{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multidimensional Scaling\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "* Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### About This Notebook\n",
    "\n",
    "Because of its similarities to PCA, this notebook will not implement multidimensional scaling \"by hand,\" although it would be easy to do by simply replicating the steps in the PCA notebook. Rather, this notebook will utilize the multidimensional scaling functionality built into scikit-learn.\n",
    "\n",
    "### About This Dataset\n",
    "\n",
    "This dataset is the NIST handwritten digit classification data set, a data set popular for its simplicity, and for being a nice introduction to image classification for machine learning appications.\n",
    "\n",
    "### About Multidimensional Scaling\n",
    "\n",
    "Suppose we have a set of $N$ points, and we know the distance between pairs of points $P_{ij}$ for $i, j = 1 \\dots N$. However, we do not know the coordinates of the points or how the distances are calculated. Multidimensional scaling attempts to lower the dimensionality of the data set by reducing the number of dimensions, subject to the restriction that the distances between each of the points should be preserved in the lower dimensional space.\n",
    "\n",
    "Thus, this is a method focused on preserving the norm of the distance between various points (preserving their relationships), rather than explaining the most amount of variance in the data possible. \n",
    "\n",
    "Suppose there is a sample $\\mathbf{X}$ with $N$ points, and we're talking about a $d$-dimensional parameter space, $\\mathbf{X}_i \\in \\mathfrak{R}^d$. In our case, the points are observations and the $d$-dimensional space is the pixel data. \n",
    "\n",
    "Then the the square of the Euclidean distance between two points $i$ and $j$ is:\n",
    "\n",
    "$$\n",
    "d_{ij}^2 = \\| \\mathbf{X}_i - \\mathbf{X}_j \\|^2\n",
    "$$\n",
    "\n",
    "We construct a matrix from the distance metric $\\mathbf{B}$, similar to how PCA constructs a covariance matrix $\\mathbf{C}$ from the variance metric. We can compute the eigenvalues of this distance matrix, $\\mathbf{B}$\n",
    "\n",
    "We find the eigenvalues and eigenvectors of $\\mathbf{B}$, denoted $\\lambda_j$ and $\\mathbf{c}_j$, respectively. Next, we decide on a number of reduced dimensions $k$. Then the new dimension $z_j$ of our multidimensional scaling (where $j$ indexes the number of reduced dimensions $k$) is:\n",
    "\n",
    "$$\n",
    "\\mathbf{z}_j = \\sqrt{\\lambda_j} \\mathbf{c}_j\n",
    "$$\n",
    "\n",
    "where $j = 1, \\dots, k$ is the index over the number of _reduced_ dimensions.\n",
    "\n",
    "We can see that under the hood, PCA and MDS are very similar: they both perform an eigenvalue analysis of a matrix formed using a metric (covariance for PCA, Euclidean distances for MDS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "# numbers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# utils\n",
    "import os, re\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "To load the data, we'll follow steps in the PCA notebook, including standardizing and normalizing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## No learning, no testing.\n",
    "#testing_df = pd.read_csv('data/optdigits/optdigits.tes',header=None)\n",
    "#X_testing,  y_testing  = testing_df.loc[:,0:63],  testing_df.loc[:,64]\n",
    "\n",
    "training_df = pd.read_csv('data/optdigits/optdigits.tra',header=None)\n",
    "X_training, y_training = training_df.loc[:,0:63], training_df.loc[:,64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X_training.shape\n",
    "print y_training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_normed_mean_cov(X):\n",
    "    X_std = StandardScaler().fit_transform(X)\n",
    "    X_mean = np.mean(X_std, axis=0)\n",
    "    \n",
    "    ## Automatic:\n",
    "    #X_cov = np.cov(X_std.T)\n",
    "    \n",
    "    # Manual:\n",
    "    X_cov = (X_std - X_mean).T.dot((X_std - X_mean)) / (X_std.shape[0]-1)\n",
    "    \n",
    "    return X_std, X_mean, X_cov\n",
    "\n",
    "X_std, X_mean, X_cov = get_normed_mean_cov(X_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multidimensional Scaling: 2 Dimensions\n",
    "\n",
    "Projecting the full 64-dimensional input to 2 dimensions will lead to a loss of a lot of information - similar to trying to use only 2 principal components. However, it's still useful to do it and visualize it to see what kind of results we'll get, and extend this approach to higher dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 300\n",
    "similarities = euclidean_distances(X_std[:n_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mds = manifold.MDS(n_components=2, max_iter=1000, eps=1e-3,\n",
    "                   n_jobs=1)\n",
    "pos = mds.fit(similarities).embedding_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_cmap(n):\n",
    "    #colorz = plt.cm.cool\n",
    "    colorz = plt.get_cmap('Set1')\n",
    "    return[ colorz(float(i)/n) for i in range(n)]\n",
    "\n",
    "colorz = get_cmap(10)\n",
    "colors = [colorz[yy] for yy in y_training]\n",
    "\n",
    "fig = plt.figure(figsize=(4,4))\n",
    "plt.scatter(pos[:,0],pos[:,1],c=colors)\n",
    "f = plt.gcf()\n",
    "ax = plt.gca()\n",
    "ax.set_title('what')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mds2 = manifold.MDS(n_components=2, max_iter=1000, eps=1e-3,\n",
    "                   dissimilarity=\"precomputed\", n_jobs=1)\n",
    "pos2 = mds2.fit(similarities).embedding_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print pos2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(pos2[:,0],pos2[:,1],c=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D Manifold\n",
    "\n",
    "Using a 3-component mutlidimensional scaling model and a 3D scatterplot leads to some improvement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 400\n",
    "similarities = euclidean_distances(X_std[:n_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mds3 = manifold.MDS(n_components=4, max_iter=1000, eps=1e-4,\n",
    "                   n_jobs=1)\n",
    "pos3 = mds3.fit(similarities).embedding_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print pos3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mds4 = manifold.MDS(n_components=3, max_iter=1000, eps=1e-3,\n",
    "                   dissimilarity=\"precomputed\", n_jobs=1)\n",
    "pos4 = mds4.fit(similarities).embedding_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print pos4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cmap(n):\n",
    "    #colorz = plt.cm.cool\n",
    "    colorz = plt.get_cmap('Set1')\n",
    "    return[ colorz(float(i)/n) for i in range(n)]\n",
    "\n",
    "colorz = get_cmap(10)\n",
    "colors = [colorz[yy] for yy in y_training[:n_samples]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax1 = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax1.scatter(pos3[:,0],pos3[:,1],pos3[:,2],c=colors)\n",
    "    \n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_zlabel('z')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
